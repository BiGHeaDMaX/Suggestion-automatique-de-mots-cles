{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Approche supervisée**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour manipuler nos données\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import numpy as np  # pour faire les np.zeros\n",
    "\n",
    "# Pour les vectorisations de type bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Gensim - Pour la vectorisation de type Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Utilisé pour l'embedding avec Word2Vec\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Visualisations\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Mesures de performance des prédictions\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, hamming_loss\n",
    "from sklearn.preprocessing import MultiLabelBinarizer  # Utilisé dans la fonction de qualité des prédictions\n",
    "\n",
    "# Mesures de durée d'éxécution\n",
    "import time\n",
    "\n",
    "# Utilisé lors des sauvagardes et chargement de données/objets\n",
    "import pickle\n",
    "import os  # Aussi utilisé pour connaitre le nombre de CPU\n",
    "\n",
    "# Éviter les Warnings\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "#warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook fonctionne avec : \n",
    "\n",
    "- Python 3.10.9\n",
    "- Anaconda 23.3.1\n",
    "- Wordcloud 1.9.2\n",
    "- Gensim 4.3.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mesure de la durée d'exécution du notebook : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_notebook = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Importations des données**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>corpus</th>\n",
       "      <th>corpus_dl</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-05-08 21:22:05</td>\n",
       "      <td>[firebase, testing, jest, error, assertion, cl...</td>\n",
       "      <td>[firebase, testing, with, jest, throws, error,...</td>\n",
       "      <td>[firebase, jestjs, reactjs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-05-08 21:49:08</td>\n",
       "      <td>[block, hack, language, benefit, block, job, w...</td>\n",
       "      <td>[concurrent, block, in, hacklang, since, hack,...</td>\n",
       "      <td>[async-await, concurrency]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-05-08 21:49:57</td>\n",
       "      <td>[type, function, typescript, function, type, i...</td>\n",
       "      <td>[can, you, set, a, type, for, a, function, in,...</td>\n",
       "      <td>[typescript]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-05-08 21:51:00</td>\n",
       "      <td>[store, service, account, looking, expo, appli...</td>\n",
       "      <td>[expo, eas, submit, where, to, store, service,...</td>\n",
       "      <td>[expo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-05-08 22:32:53</td>\n",
       "      <td>[store, retrieve, structure, type, c, copying,...</td>\n",
       "      <td>[reliably, and, portably, store, and, retrieve...</td>\n",
       "      <td>[c]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46493</th>\n",
       "      <td>2022-03-21 08:52:43</td>\n",
       "      <td>[option, option, know, option, case, option, o...</td>\n",
       "      <td>[how, to, know, if, a, select2, has, options, ...</td>\n",
       "      <td>[javascript, jquery]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46494</th>\n",
       "      <td>2022-03-21 08:54:03</td>\n",
       "      <td>[ring, plot, attempt, gap, use, plot, paint, r...</td>\n",
       "      <td>[matplotlib, how, to, plot, a, closed, ring, i...</td>\n",
       "      <td>[matplotlib, python]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46495</th>\n",
       "      <td>2022-03-21 09:02:54</td>\n",
       "      <td>[security, problem, terraform, dependency, loc...</td>\n",
       "      <td>[which, security, problem, does, terraform, ch...</td>\n",
       "      <td>[terraform]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46496</th>\n",
       "      <td>2022-03-21 13:54:21</td>\n",
       "      <td>[studio, creation, blob, container, error, mes...</td>\n",
       "      <td>[visual, studio, 2022, with, azurite, integrat...</td>\n",
       "      <td>[azure]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46497</th>\n",
       "      <td>2022-03-21 13:56:22</td>\n",
       "      <td>[pattern, aw, parameter, store, array, appsett...</td>\n",
       "      <td>[use, ioption, pattern, with, aws, parameter, ...</td>\n",
       "      <td>[.net, .net-core, c#]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46498 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     date                                             corpus  \\\n",
       "0     2022-05-08 21:22:05  [firebase, testing, jest, error, assertion, cl...   \n",
       "1     2022-05-08 21:49:08  [block, hack, language, benefit, block, job, w...   \n",
       "2     2022-05-08 21:49:57  [type, function, typescript, function, type, i...   \n",
       "3     2022-05-08 21:51:00  [store, service, account, looking, expo, appli...   \n",
       "4     2022-05-08 22:32:53  [store, retrieve, structure, type, c, copying,...   \n",
       "...                   ...                                                ...   \n",
       "46493 2022-03-21 08:52:43  [option, option, know, option, case, option, o...   \n",
       "46494 2022-03-21 08:54:03  [ring, plot, attempt, gap, use, plot, paint, r...   \n",
       "46495 2022-03-21 09:02:54  [security, problem, terraform, dependency, loc...   \n",
       "46496 2022-03-21 13:54:21  [studio, creation, blob, container, error, mes...   \n",
       "46497 2022-03-21 13:56:22  [pattern, aw, parameter, store, array, appsett...   \n",
       "\n",
       "                                               corpus_dl  \\\n",
       "0      [firebase, testing, with, jest, throws, error,...   \n",
       "1      [concurrent, block, in, hacklang, since, hack,...   \n",
       "2      [can, you, set, a, type, for, a, function, in,...   \n",
       "3      [expo, eas, submit, where, to, store, service,...   \n",
       "4      [reliably, and, portably, store, and, retrieve...   \n",
       "...                                                  ...   \n",
       "46493  [how, to, know, if, a, select2, has, options, ...   \n",
       "46494  [matplotlib, how, to, plot, a, closed, ring, i...   \n",
       "46495  [which, security, problem, does, terraform, ch...   \n",
       "46496  [visual, studio, 2022, with, azurite, integrat...   \n",
       "46497  [use, ioption, pattern, with, aws, parameter, ...   \n",
       "\n",
       "                              tags  \n",
       "0      [firebase, jestjs, reactjs]  \n",
       "1       [async-await, concurrency]  \n",
       "2                     [typescript]  \n",
       "3                           [expo]  \n",
       "4                              [c]  \n",
       "...                            ...  \n",
       "46493         [javascript, jquery]  \n",
       "46494         [matplotlib, python]  \n",
       "46495                  [terraform]  \n",
       "46496                      [azure]  \n",
       "46497        [.net, .net-core, c#]  \n",
       "\n",
       "[46498 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data_clean.pkl', 'rb') as fichier:\n",
    "    data = pickle.load(fichier)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Option d'échantillonages pour la réalisation de tests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>corpus</th>\n",
       "      <th>corpus_dl</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-05-08 21:22:05</td>\n",
       "      <td>[firebase, testing, jest, error, assertion, cl...</td>\n",
       "      <td>[firebase, testing, with, jest, throws, error,...</td>\n",
       "      <td>[firebase, jestjs, reactjs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-05-08 21:49:08</td>\n",
       "      <td>[block, hack, language, benefit, block, job, w...</td>\n",
       "      <td>[concurrent, block, in, hacklang, since, hack,...</td>\n",
       "      <td>[async-await, concurrency]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-05-08 21:49:57</td>\n",
       "      <td>[type, function, typescript, function, type, i...</td>\n",
       "      <td>[can, you, set, a, type, for, a, function, in,...</td>\n",
       "      <td>[typescript]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-05-08 21:51:00</td>\n",
       "      <td>[store, service, account, looking, expo, appli...</td>\n",
       "      <td>[expo, eas, submit, where, to, store, service,...</td>\n",
       "      <td>[expo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-05-08 22:32:53</td>\n",
       "      <td>[store, retrieve, structure, type, c, copying,...</td>\n",
       "      <td>[reliably, and, portably, store, and, retrieve...</td>\n",
       "      <td>[c]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46493</th>\n",
       "      <td>2022-03-21 08:52:43</td>\n",
       "      <td>[option, option, know, option, case, option, o...</td>\n",
       "      <td>[how, to, know, if, a, select2, has, options, ...</td>\n",
       "      <td>[javascript, jquery]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46494</th>\n",
       "      <td>2022-03-21 08:54:03</td>\n",
       "      <td>[ring, plot, attempt, gap, use, plot, paint, r...</td>\n",
       "      <td>[matplotlib, how, to, plot, a, closed, ring, i...</td>\n",
       "      <td>[matplotlib, python]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46495</th>\n",
       "      <td>2022-03-21 09:02:54</td>\n",
       "      <td>[security, problem, terraform, dependency, loc...</td>\n",
       "      <td>[which, security, problem, does, terraform, ch...</td>\n",
       "      <td>[terraform]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46496</th>\n",
       "      <td>2022-03-21 13:54:21</td>\n",
       "      <td>[studio, creation, blob, container, error, mes...</td>\n",
       "      <td>[visual, studio, 2022, with, azurite, integrat...</td>\n",
       "      <td>[azure]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46497</th>\n",
       "      <td>2022-03-21 13:56:22</td>\n",
       "      <td>[pattern, aw, parameter, store, array, appsett...</td>\n",
       "      <td>[use, ioption, pattern, with, aws, parameter, ...</td>\n",
       "      <td>[.net, .net-core, c#]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46498 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     date                                             corpus  \\\n",
       "0     2022-05-08 21:22:05  [firebase, testing, jest, error, assertion, cl...   \n",
       "1     2022-05-08 21:49:08  [block, hack, language, benefit, block, job, w...   \n",
       "2     2022-05-08 21:49:57  [type, function, typescript, function, type, i...   \n",
       "3     2022-05-08 21:51:00  [store, service, account, looking, expo, appli...   \n",
       "4     2022-05-08 22:32:53  [store, retrieve, structure, type, c, copying,...   \n",
       "...                   ...                                                ...   \n",
       "46493 2022-03-21 08:52:43  [option, option, know, option, case, option, o...   \n",
       "46494 2022-03-21 08:54:03  [ring, plot, attempt, gap, use, plot, paint, r...   \n",
       "46495 2022-03-21 09:02:54  [security, problem, terraform, dependency, loc...   \n",
       "46496 2022-03-21 13:54:21  [studio, creation, blob, container, error, mes...   \n",
       "46497 2022-03-21 13:56:22  [pattern, aw, parameter, store, array, appsett...   \n",
       "\n",
       "                                               corpus_dl  \\\n",
       "0      [firebase, testing, with, jest, throws, error,...   \n",
       "1      [concurrent, block, in, hacklang, since, hack,...   \n",
       "2      [can, you, set, a, type, for, a, function, in,...   \n",
       "3      [expo, eas, submit, where, to, store, service,...   \n",
       "4      [reliably, and, portably, store, and, retrieve...   \n",
       "...                                                  ...   \n",
       "46493  [how, to, know, if, a, select2, has, options, ...   \n",
       "46494  [matplotlib, how, to, plot, a, closed, ring, i...   \n",
       "46495  [which, security, problem, does, terraform, ch...   \n",
       "46496  [visual, studio, 2022, with, azurite, integrat...   \n",
       "46497  [use, ioption, pattern, with, aws, parameter, ...   \n",
       "\n",
       "                              tags  \n",
       "0      [firebase, jestjs, reactjs]  \n",
       "1       [async-await, concurrency]  \n",
       "2                     [typescript]  \n",
       "3                           [expo]  \n",
       "4                              [c]  \n",
       "...                            ...  \n",
       "46493         [javascript, jquery]  \n",
       "46494         [matplotlib, python]  \n",
       "46495                  [terraform]  \n",
       "46496                      [azure]  \n",
       "46497        [.net, .net-core, c#]  \n",
       "\n",
       "[46498 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "echantillon = False\n",
    "taille_echantillon = 5000\n",
    "\n",
    "if echantillon:\n",
    "    data = data.sample(taille_echantillon, random_state=42)  # random_state car en fonction de l'échantillonage\n",
    "                                                             # les résultats peuvent changer et pour pouvoir\n",
    "                                                             # comparer nos tunings d'hyperparamètres, nous devons\n",
    "                                                             # avoir des résultats reproductibles\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Vectorisations du corpus**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Vectorisations de type *bag-of-words* classique**\n",
    "\n",
    "Nous utilisons *data['corpus']*, le corpus avec traitement complet (suppression des stopwords, des mots rares, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'observations (documents) : 46498\n",
      "Nombre de variables (features) : 500\n",
      "Premier document vectorisé (100 premières features) :\n",
      "[0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Chaque document de data['corpus'] est sous la forme d'une liste de mot.\n",
    "# CountVectorizer() prend en entrée un string. Nous allons donc concaténer chaque document.\n",
    "preprocessed_corpus = [' '.join(doc) for doc in data['corpus']]\n",
    "\n",
    "# max_features : ne retenir que les x mots les plus fréquents\n",
    "# Sans cette limite, le nombre de features serait égal\n",
    "# au nombre de mots dans le vocabulaire\n",
    "max_features = 500\n",
    "\n",
    "# Création de CountVectorizer et vectorisation des données textuelles\n",
    "vectorizer_count = CountVectorizer(max_features=max_features)\n",
    "vectorized_corpus_count = vectorizer_count.fit_transform(preprocessed_corpus)\n",
    "\n",
    "# Afficher les caractéristiques de la matrice vectorisée\n",
    "print(f\"Nombre d'observations (documents) : {vectorized_corpus_count.shape[0]}\")\n",
    "print(f\"Nombre de variables (features) : {vectorized_corpus_count.shape[1]}\")\n",
    "print(\"Premier document vectorisé (100 premières features) :\")\n",
    "print(vectorized_corpus_count.toarray()[0][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Vectorisations de type *bag-of-words* TF-IDF**\n",
    "\n",
    "Nous utilisons encore *data['corpus']*, le corpus avec traitement complet (suppression des stopwords, des mots rares, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'observations (documents) : 46498\n",
      "Nombre de variables (features) : 500\n",
      "Premier document vectorisé (100 premières features) :\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.20109768\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.16017539 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.09734    0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.11521863\n",
      " 0.         0.         0.         0.13373622 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "# max_features : ne retenir que les x mots les plus fréquents\n",
    "# Sans cette limite, le nombre de features serait égal\n",
    "# au nombre de mots dans le vocabulaire\n",
    "max_features = 500\n",
    "\n",
    "# Création de TfidfVectorizer et vectorisation des données textuelles\n",
    "vectorizer_tfidf = TfidfVectorizer(max_features=max_features)\n",
    "vectorized_corpus_ftidf = vectorizer_tfidf.fit_transform(preprocessed_corpus)  # On réutilise preprocessed_corpus crée lors\n",
    "                                                                               # de la vectorisation BoW classique\n",
    "\n",
    "# Afficher les caractéristiques de la matrice vectorisée\n",
    "print(f\"Nombre d'observations (documents) : {vectorized_corpus_ftidf.shape[0]}\")\n",
    "print(f\"Nombre de variables (features) : {vectorized_corpus_ftidf.shape[1]}\")\n",
    "print(\"Premier document vectorisé (100 premières features) :\")\n",
    "print(vectorized_corpus_ftidf.toarray()[0][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Vectorisations de type *Word/Sentence Embedding***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **A). Word2Vec**\n",
    "\n",
    "Nous utilisons de nouveau *data['corpus']*, le corpus avec traitement complet (suppression des stopwords, des mots rares, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant de démarrer, regardons les tailles des documents de notre corpus avec traitement complet (data['corpus']), ceci va nous servir par la suite : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La taille du plus court document est : 1\n",
      "La taille du plus long document est : 846\n",
      "La taille moyenne des documents est : 26.3\n"
     ]
    }
   ],
   "source": [
    "min_length = min(len(document) for document in data['corpus'])\n",
    "\n",
    "# Afficher la taille de la plus grande liste\n",
    "print(f\"La taille du plus court document est : {min_length}\")\n",
    "\n",
    "max_length = max(len(document) for document in data['corpus'])\n",
    "\n",
    "# Afficher la taille de la plus grande liste\n",
    "print(f\"La taille du plus long document est : {max_length}\")\n",
    "\n",
    "# Calculer la taille moyenne\n",
    "average_length = sum(len(document) for document in data['corpus']) / len(data['corpus'])\n",
    "\n",
    "# Afficher la taille moyenne\n",
    "print(f\"La taille moyenne des documents est : {average_length:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Choix entre CBOW et Skip-gram à utiliser dans Word2Vec.\n",
    "\n",
    "**CBOW (Continuous Bag of Words) :** vise à prédire un mot étant donné son contexte, c'est-à-dire étant donné les mots qui en sont proches dans le texte. S'entraîne plus rapidement que Skip-Gram (mieux adapté pour des dataset volumineux) et peut mieux représenter les mots plus fréquents.\n",
    "\n",
    "**Skip-gram :** architecture symétrique visant à prédire les mots du contexte étant donné un mot en entrée. Fonctionne bien avec de petits datsets et peut mieux représenter les mots moins fréquents.\n",
    "\n",
    "Dans notre cas, notamment en raison de la taille du dataset, nous utiliserons **CBOW**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Création et entraînement du modèle Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Création et entraînement du modèle Word2Vec...\n",
      "Taille du vocabulaire : 3707\n",
      "Word2Vec entraîné\n"
     ]
    }
   ],
   "source": [
    "# Paramètres à passer dans le modèle : \n",
    "\n",
    "w2v_size = 100  # Taille max des vecteurs\n",
    "w2v_window = 5  # Taille du contexte\n",
    "w2v_min_count = 1  # Minimum d'occurences d'un mot pour être pris en compte\n",
    "w2v_epochs = 50  # Nombre de passes sur tout le corpus\n",
    "\n",
    "documents = data['corpus']\n",
    "\n",
    "# Création et entraînement du modèle Word2Vec\n",
    "\n",
    "print(\"Création et entraînement du modèle Word2Vec...\")\n",
    "w2v_model = Word2Vec(min_count=w2v_min_count,\n",
    "                     window=w2v_window,\n",
    "                     vector_size=w2v_size,\n",
    "                     sg=0,  # 0 : CBOX, 1 : Skipgram\n",
    "                     seed=42,\n",
    "                     workers=(os.cpu_count() - 1)  # Utiliser total des cpu - 1\n",
    ")\n",
    "\n",
    "w2v_model.build_vocab(documents)\n",
    "\n",
    "w2v_model.train(documents,\n",
    "                total_examples=w2v_model.corpus_count,\n",
    "                epochs=w2v_epochs\n",
    ")\n",
    "\n",
    "model_vectors = w2v_model.wv\n",
    "\n",
    "w2v_words = model_vectors.index_to_key\n",
    "\n",
    "print(\"Taille du vocabulaire : %i\" % len(w2v_words))\n",
    "print(\"Word2Vec entraîné\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test du modèle entraîné pour vérifier qu'il a réussi à capter les similarités entre les mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mots similaires à 'python' :\n",
      "\n",
      "pip : 0.4794\n",
      "modulenotfounderror : 0.4427\n",
      "conda : 0.4255\n",
      "poetry : 0.4159\n",
      "venv : 0.4156\n",
      "pypi : 0.4057\n",
      "setuptool : 0.4013\n",
      "pyenv : 0.4010\n",
      "interpreter : 0.3994\n",
      "gdal : 0.3952\n",
      "\n",
      "--------------------\n",
      "\n",
      "Similarité entre 'python' et 'virtualenv' : 0.3547\n",
      "\n",
      "--------------------\n",
      "\n",
      "L'intrus parmi ces mots ['python', 'javascript', 'image'] est : 'image'.\n"
     ]
    }
   ],
   "source": [
    "# Mots similaires à un mot donné\n",
    "mot_a_tester = \"python\"\n",
    "print(f\"Mots similaires à '{mot_a_tester}' :\\n\")\n",
    "similar_words = w2v_model.wv.most_similar(positive=[mot_a_tester])\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word} : {similarity:.4f}\")\n",
    "\n",
    "print(\"\\n--------------------\\n\")\n",
    "\n",
    "# Similarité entre deux mots\n",
    "mot_a_tester_1 = \"python\"\n",
    "mot_a_tester_2 = \"virtualenv\"\n",
    "similarite = w2v_model.wv.similarity(mot_a_tester_1, mot_a_tester_2)\n",
    "print(f\"Similarité entre '{mot_a_tester_1}' et '{mot_a_tester_2}' : {similarite:.4f}\")\n",
    "\n",
    "print(\"\\n--------------------\\n\")\n",
    "\n",
    "# Trouver l'intrus\n",
    "mot_intrus_1 = \"python\"\n",
    "mot_intrus_2 = \"javascript\"\n",
    "mot_intrus_3 = \"image\"\n",
    "intrus = w2v_model.wv.doesnt_match([mot_intrus_1, mot_intrus_2, mot_intrus_3])\n",
    "print(f\"L'intrus parmi ces mots ['{mot_intrus_1}', '{mot_intrus_2}', '{mot_intrus_3}'] est : '{intrus}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Cela semble assez cohérent. Le modèle semble avoir réussi à capter les similarités entre les mots.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Préparation des documents (tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit Tokenizer ...\n",
      "Nombre de mots uniques : 3708\n",
      "Premier document encodé :\n",
      "[ 290 1561    2  935   13  222  290   27   11  245   36   39  161   40\n",
      "  245    2   36 1627  245  167    3  245   66   27  313  463]\n"
     ]
    }
   ],
   "source": [
    "maxlen = int(average_length)  # on va prendre ici la taille moyenne (en nombre de mot)\n",
    "                              # des documents de notre corpus, calculée précédemment,\n",
    "                              # que j'arrondis en entier pour éviter les erreurs\n",
    "\n",
    "# On réutilise documents (défini lors de la création et entraînement du modèle Word2Vec),\n",
    "# documents contient data['corpus']\n",
    "print(\"Fit Tokenizer ...\")\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(documents)\n",
    "x_documents = pad_sequences(tokenizer.texts_to_sequences(documents),\n",
    "                            maxlen=maxlen,  # On ne prend que les maxlen premiers mots de chaque document\n",
    "                            padding='post'  # Si le document est plus petit que maxlen, on complète avec des 0\n",
    ") \n",
    "                                                   \n",
    "num_words = len(tokenizer.word_index) + 1  # + 1 : on ajoute 1 pour inclure un index supplémentaire.\n",
    "                                           # Cet index supplémentaire est ajouté pour tenir compte des mots inconnus\n",
    "                                           # qui n'apparaissent pas dans le jeu de données d'entraînement initial.\n",
    "                                           \n",
    "print(\"Nombre de mots uniques : %i\" % num_words)\n",
    "print(\"Premier document encodé :\")\n",
    "print(x_documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Création de la matrice d'embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Création de la matrice d'embedding...\n",
      "Taux d'intégration de mots : 1.0\n",
      "Dimensions de la matrice d'embedding : (3708, 100)\n"
     ]
    }
   ],
   "source": [
    "print(\"Création de la matrice d'embedding...\")\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index) + 1  # + 1 pour les même raisons que précédemment.\n",
    "embedding_matrix = np.zeros((vocab_size, w2v_size))  # w2v_size a été déterminé à la création et\n",
    "                                                     # entraînement du modèle Word2Vec, pour représenter\n",
    "                                                     # la taille max des vecteurs. La matrice qu'on crée ici\n",
    "                                                     # doit avoir la même \"largeur\".\n",
    "\n",
    "i=0\n",
    "j=0   \n",
    "for word, idx in word_index.items():\n",
    "    i +=1\n",
    "    if word in w2v_words:\n",
    "        j +=1\n",
    "        embedding_vector = model_vectors[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx] = model_vectors[word]\n",
    "            \n",
    "word_rate = np.round(j/i,4)  # Taux d'intégration de mots : Ce taux indique la proportion de mots du corpus\n",
    "                             # pour lesquels un vecteur d'embedding a été trouvé dans le modèle Word2Vec.\n",
    "                             # Un taux élevé suggère que la plupart des mots du corpus ont été trouvés dans\n",
    "                             # le modèle Word2Vec, ce qui signifie que la couverture du modèle d'embedding\n",
    "                             # est suffisamment large pour représenter le vocabulaire du corpus. \n",
    "\n",
    "print(\"Taux d'intégration de mots :\", word_rate)\n",
    "print(f\"Dimensions de la matrice d'embedding : {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le taux d'intégration de mots est bon, la couverture du modèle d'embedding est suffisamment large pour représenter le vocabulaire du corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Création du modèle d'embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 26)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 26, 100)           370800    \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 100)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 370,800\n",
      "Trainable params: 370,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Création du modèle\n",
    "\n",
    "input = Input(shape=(len(x_documents), maxlen),  # On réutilise le même maxlen\n",
    "                                                 # que celui défini lors de la préparation\n",
    "                                                 # des documents (tokenization)\n",
    "            dtype='float64'\n",
    ")\n",
    "\n",
    "word_input = Input(shape=(maxlen,),dtype='float64')\n",
    "\n",
    "word_embedding = Embedding(input_dim=vocab_size,\n",
    "                           output_dim=w2v_size,\n",
    "                           weights = [embedding_matrix],\n",
    "                           input_length=maxlen)(word_input)\n",
    "\n",
    "word_vec=GlobalAveragePooling1D()(word_embedding)\n",
    "\n",
    "embed_model = Model([word_input],word_vec)\n",
    "\n",
    "embed_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Exécution du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions de 'word2vec_embedded' : (46498, 100)\n",
      "Premier document encodé :\n",
      "[-0.40545294 -0.09687266  0.19511618 -0.00361695  0.01519568 -0.23669513\n",
      "  0.9939283   0.60402423 -0.01854311  0.39658058  0.71295476  0.32810813\n",
      " -0.83919275 -0.41721785 -0.6430797   0.16601823  0.16116169  0.88745284\n",
      "  0.14624248  0.05588098  1.0842241   0.3058086  -0.35636872 -0.7434487\n",
      "  0.0683717   0.28726614  0.44141242 -0.73166156 -0.05949076 -0.6878446\n",
      "  0.5763946  -0.97624964  0.27273828  0.3472854  -0.18710653 -0.18883404\n",
      " -0.7774018  -0.68133426 -0.02940808  0.00290322 -0.19713283  0.11295965\n",
      " -1.1706913  -0.32782844  0.33893853  0.8681342  -0.31117445  0.07136986\n",
      " -0.44696397 -0.67729694  0.33093786  0.38690192  1.1631516   0.8139627\n",
      "  0.50896585 -0.28680724  0.7326416   0.11956118  0.28780574  0.11909994\n",
      "  0.504765    0.2388108  -0.56621665  0.520311    0.07890415 -0.0025532\n",
      "  0.3769908   0.16106261 -0.3455995  -0.21254331  0.5828712  -0.33095977\n",
      "  0.0181898   0.38551027  0.61834896  0.0210034   0.69519234  0.5575342\n",
      "  0.27351177 -0.6998916  -0.1278167   0.4150267   0.2985621  -0.16961084\n",
      " -0.3750167  -0.585772   -0.72070974  0.607072   -0.57295626  0.32791534\n",
      "  0.5031381  -0.13646701 -0.2128117  -0.489848    0.09612472  0.06216727\n",
      " -0.4377281  -0.9364571   0.8624615   0.54404473]\n"
     ]
    }
   ],
   "source": [
    "word2vec_embedded = embed_model.predict(x_documents, verbose=0)\n",
    "print(f\"Dimensions de 'word2vec_embedded' : {word2vec_embedded.shape}\")\n",
    "print(\"Premier document encodé :\")\n",
    "print(word2vec_embedded[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
